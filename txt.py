import io
import time
from datetime import datetime
from typing import Tuple

import numpy as np
from PIL import Image

import streamlit as st

# --- Optional: suppress torch compile warnings on CPU only environments ---
st.set_page_config(page_title="AI Style Transfer", page_icon="ğŸ¨", layout="wide")

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import torchvision.transforms as T
    from torchvision.models import vgg19, VGG19_Weights
    TORCH_OK = True
except Exception as e:
    TORCH_OK = False
    TORCH_ERR = e

# =============================
# Utilities
# =============================

def pil_to_tensor(img: Image.Image, max_size: int = 512) -> torch.Tensor:
    """Convert PIL to normalized tensor [1,3,H,W] in 0..1."""
    if img.mode != "RGB":
        img = img.convert("RGB")
    w, h = img.size
    scale = min(1.0, max_size / max(w, h))
    if scale < 1.0:
        img = img.resize((int(w * scale), int(h * scale)), Image.LANCZOS)
    tfm = T.Compose([
        T.ToTensor(),  # [0,1]
    ])
    t = tfm(img).unsqueeze(0)
    return t


def tensor_to_pil(t: torch.Tensor) -> Image.Image:
    t = t.detach().cpu().clamp(0, 1)
    return T.ToPILImage()(t.squeeze(0))


class Normalization(nn.Module):
    def __init__(self, mean, std):
        super().__init__()
        # reshape to [C,1,1]
        self.mean = torch.tensor(mean).view(-1, 1, 1)
        self.std = torch.tensor(std).view(-1, 1, 1)

    def forward(self, img):
        return (img - self.mean) / self.std


def gram_matrix(x: torch.Tensor) -> torch.Tensor:
    b, c, h, w = x.size()
    features = x.view(b * c, h * w)
    G = torch.mm(features, features.t())
    return G.div(b * c * h * w)


class ContentLoss(nn.Module):
    def __init__(self, target: torch.Tensor, weight: float = 1.0):
        super().__init__()
        self.target = target.detach()
        self.weight = weight
        self.loss = torch.tensor(0.0)

    def forward(self, x):
        self.loss = self.weight * nn.functional.mse_loss(x, self.target)
        return x


class StyleLoss(nn.Module):
    def __init__(self, target_feature: torch.Tensor, weight: float = 1.0):
        super().__init__()
        self.target = gram_matrix(target_feature).detach()
        self.weight = weight
        self.loss = torch.tensor(0.0)

    def forward(self, x):
        G = gram_matrix(x)
        self.loss = self.weight * nn.functional.mse_loss(G, self.target)
        return x


def build_style_transfer_model(cnn: nn.Module,
                               normalization_mean,
                               normalization_std,
                               style_img: torch.Tensor,
                               content_img: torch.Tensor,
                               style_layers=None,
                               content_layers=None,
                               style_weight: float = 1e6,
                               content_weight: float = 1.0,
                               device: str = "cpu"):
    if style_layers is None:
        # Typical layers from Gatys paper / PyTorch tutorial
        style_layers = ["conv1_1", "conv2_1", "conv3_1", "conv4_1", "conv5_1"]
    if content_layers is None:
        content_layers = ["conv4_2"]

    normalization = Normalization(normalization_mean, normalization_std).to(device)

    content_losses = []
    style_losses = []

    model = nn.Sequential(normalization)

    i = 0  # increment for conv layers
    for layer in cnn.features.children():
        if isinstance(layer, nn.Conv2d):
            i += 1
            name = f"conv{i}_1" if i not in [1,2,3,4,5] else f"conv{i}_1"  # simple naming per block's first conv
        elif isinstance(layer, nn.ReLU):
            name = f"relu{i}"
            layer = nn.ReLU(inplace=False)
        elif isinstance(layer, nn.MaxPool2d):
            name = f"pool{i}"
        elif isinstance(layer, nn.BatchNorm2d):
            name = f"bn{i}"
        else:
            name = f"layer_{len(model)}"
        model.add_module(name, layer)

        if name in content_layers:
            target = model(content_img).detach()
            content_loss = ContentLoss(target, content_weight)
            model.add_module(f"content_loss_{i}", content_loss)
            content_losses.append(content_loss)

        if name in style_layers:
            target_feature = model(style_img).detach()
            style_loss = StyleLoss(target_feature, style_weight)
            model.add_module(f"style_loss_{i}", style_loss)
            style_losses.append(style_loss)

    # Trim the model after the last loss to speed up
    for idx in range(len(model) - 1, -1, -1):
        if isinstance(model[idx], (ContentLoss, StyleLoss)):
            model = model[: idx + 1]
            break

    return model, style_losses, content_losses


def run_style_transfer(content_img: Image.Image,
                       style_img: Image.Image,
                       max_size: int = 512,
                       num_steps: int = 300,
                       style_weight: float = 1e6,
                       content_weight: float = 1.0,
                       lr: float = 0.03,
                       device: str = "cpu") -> Image.Image:
    """Run neural style transfer using Adam optimizer on the input image."""
    device = torch.device(device)

    content_t = pil_to_tensor(content_img, max_size=max_size).to(device)
    style_t = pil_to_tensor(style_img, max_size=max_size).to(device)

    # Start from content image for faster convergence
    input_img = content_t.clone().requires_grad_(True)

    # Load VGG19 weights
    weights = VGG19_Weights.DEFAULT
    cnn = vgg19(weights=weights).to(device).eval()

    # Normalization stats expected by VGG19
    norm_mean = weights.meta["mean"]
    norm_std = weights.meta["std"]

    model, style_losses, content_losses = build_style_transfer_model(
        cnn, norm_mean, norm_std, style_t, content_t,
        style_weight=style_weight, content_weight=content_weight, device=str(device)
    )

    optimizer = optim.Adam([input_img], lr=lr)

    pbar = st.progress(0, text="ìŠ¤íƒ€ì¼ ë³€í™˜ ì§„í–‰ ì¤‘â€¦")
    status = st.empty()

    for step in range(1, num_steps + 1):
        optimizer.zero_grad()
        model(input_img)
        style_score = sum(sl.loss for sl in style_losses)
        content_score = sum(cl.loss for cl in content_losses)
        loss = style_score + content_score
        loss.backward()
        optimizer.step()

        if step % max(1, num_steps // 100) == 0:
            pbar.progress(step / num_steps, text=f"Step {step}/{num_steps} | Style: {style_score.item():.2f} | Content: {content_score.item():.2f}")
            status.text(f"Style Loss: {style_score.item():.2f} | Content Loss: {content_score.item():.2f}")

    result = tensor_to_pil(input_img)
    pbar.empty()
    status.empty()
    return result


# =============================
# UI
# =============================

st.title("ğŸ¨ AI Style Transfer (Neural Style Transfer)")
st.caption("ì»¨í…ì¸  ì´ë¯¸ì§€ì— ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€ë¥¼ ì…í˜€ ìƒˆë¡œìš´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ì•±ì…ë‹ˆë‹¤. PyTorch + VGG19")

if not TORCH_OK:
    st.error("PyTorch/torchvisionì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì•„ë˜ ì˜¤ë¥˜ë¥¼ í™•ì¸í•˜ê³  í™˜ê²½ì„ ì¤€ë¹„í•´ ì£¼ì„¸ìš”.\n\n" + str(TORCH_ERR))
    st.stop()

with st.sidebar:
    st.header("â‘  ì´ë¯¸ì§€ ì—…ë¡œë“œ")
    content_file = st.file_uploader("ì»¨í…ì¸  ì´ë¯¸ì§€ (ì‚¬ì§„)", type=["jpg", "jpeg", "png"])
    style_file = st.file_uploader("ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€ (ê·¸ë¦¼/íŒ¨í„´)", type=["jpg", "jpeg", "png"])

    st.divider()
    st.header("â‘¡ íŒŒë¼ë¯¸í„°")
    max_size = st.slider("ìµœëŒ€ í•´ìƒë„ (ê¸´ë³€)", 256, 1024, 512, step=32, help="í´ìˆ˜ë¡ ì„ ëª…í•˜ì§€ë§Œ ëŠë ¤ì§‘ë‹ˆë‹¤.")
    num_steps = st.slider("ë°˜ë³µ íšŸìˆ˜ (steps)", 50, 800, 300, step=50)
    style_weight = st.number_input("Style Weight", value=1e6, min_value=1e3, max_value=1e8, step=1e5, format="%.0f")
    content_weight = st.number_input("Content Weight", value=1.0, min_value=0.0001, max_value=10.0, step=0.1, format="%.4f")
    lr = st.number_input("Learning Rate (Adam)", value=0.03, min_value=0.001, max_value=0.5, step=0.01, format="%.3f")

    st.divider()
    device_opt = "cuda" if torch.cuda.is_available() else "cpu"
    device = st.selectbox("ì¥ì¹˜ (Device)", [device_opt, "cpu", "cuda"], index=0 if device_opt=="cuda" else 1, help="GPUê°€ ìˆë‹¤ë©´ cudaë¥¼ ì„ íƒí•˜ì„¸ìš”.")

st.markdown("""
**ì‚¬ìš© ë°©ë²•**
1) ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì—ì„œ ì»¨í…ì¸  ì´ë¯¸ì§€(ì‚¬ì§„)ì™€ ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€(ê·¸ë¦¼/íŒ¨í„´)ë¥¼ ì—…ë¡œë“œí•©ë‹ˆë‹¤.  
2) í•´ìƒë„/ë°˜ë³µ íšŸìˆ˜/ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤.  
3) âœ¨ **'ìŠ¤íƒ€ì¼ ë³€í™˜ ì‹¤í–‰'** ë²„íŠ¼ì„ ëˆ„ë¦…ë‹ˆë‹¤.
""")

col1, col2 = st.columns(2)
with col1:
    st.subheader("ì»¨í…ì¸  ì´ë¯¸ì§€")
    if content_file:
        content_img = Image.open(content_file)
        st.image(content_img, caption="Content", use_column_width=True)
    else:
        content_img = None
        st.info("ì‚¬ì§„ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.")

with col2:
    st.subheader("ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€")
    if style_file:
        style_img = Image.open(style_file)
        st.image(style_img, caption="Style", use_column_width=True)
    else:
        style_img = None
        st.info("ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.")

st.divider()

run_btn = st.button("âœ¨ ìŠ¤íƒ€ì¼ ë³€í™˜ ì‹¤í–‰", type="primary", use_container_width=True)

if run_btn:
    if content_img is None or style_img is None:
        st.warning("ë‘ ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        st.stop()
    start = time.time()
    result = run_style_transfer(
        content_img=content_img,
        style_img=style_img,
        max_size=max_size,
        num_steps=int(num_steps),
        style_weight=float(style_weight),
        content_weight=float(content_weight),
        lr=float(lr),
        device=device,
    )
    elapsed = time.time() - start
    st.success(f"ì™„ë£Œ! ê²½ê³¼ ì‹œê°„: {elapsed:.1f}ì´ˆ")
    st.image(result, caption="Result", use_column_width=True)

    # Downloads
    buf_png = io.BytesIO()
    result.save(buf_png, format="PNG")
    buf_png.seek(0)

    buf_jpg = io.BytesIO()
    result.convert("RGB").save(buf_jpg, format="JPEG", quality=95)
    buf_jpg.seek(0)

    c1, c2 = st.columns(2)
    with c1:
        st.download_button(
            "â¬‡ï¸ PNG ë‹¤ìš´ë¡œë“œ",
            data=buf_png,
            file_name=f"style_transfer_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png",
            mime="image/png",
            use_container_width=True,
        )
    with c2:
        st.download_button(
            "â¬‡ï¸ JPG ë‹¤ìš´ë¡œë“œ",
            data=buf_jpg,
            file_name=f"style_transfer_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jpg",
            mime="image/jpeg",
            use_container_width=True,
        )

st.divider()

st.markdown(
    """
### âš™ï¸ ì„¤ì¹˜
```bash
pip install streamlit torch torchvision pillow
```

### ğŸ’¡ íŒ
- ê²°ê³¼ê°€ ë­‰ê°œì§€ë©´: **ë°˜ë³µ íšŸìˆ˜ ì¦ê°€** ë˜ëŠ” **LR ê°ì†Œ(0.01~0.03)**
- ìŠ¤íƒ€ì¼ì´ ë„ˆë¬´ ì„ë©´: **Style Weight ê°ì†Œ** ë˜ëŠ” **Content Weight ì¦ê°€**
- ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸: í•´ìƒë„ 384, Steps 150ë¶€í„° ì‹œì‘
- GPU ì‚¬ìš© ì‹œ: CUDAê°€ ì„¤ì¹˜ëœ í™˜ê²½ì—ì„œ ì¥ì¹˜ ì˜µì…˜ì„ `cuda`ë¡œ ì„ íƒ

### ğŸ“Œ ì°¸ê³ 
- ë³¸ ì½”ë“œëŠ” VGG19 ì‚¬ì „í•™ìŠµ íŠ¹ì§•ì„ ì´ìš©í•œ **Neural Style Transfer (Gatys et al.)** êµ¬í˜„ì…ë‹ˆë‹¤.
"""
)
